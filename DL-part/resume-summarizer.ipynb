{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\EDWIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text # type: ignore\n",
    "import nltk # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import re\n",
    " \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDWIN CHAZHOOR\n",
      "\n",
      "+91-7069940011  • edwinchazhoor0408@gmail.com • Github: https://github.com/edwin16804 \n",
      "• LinkedIn: https://www.linkedin.com/in/edwin-chazhoor/\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "BTech. Computer Science Engineering with specialization in AIML\n",
      "Vellore Institute of Technology-Chennai\n",
      "\n",
      "Jul 2022- Present\n",
      "\n",
      "CGPA: 8.97\n",
      "\n",
      "CBSE - XII\n",
      "Anand Vidya Vihar- Vadodara\n",
      "\n",
      "Result: 92%\n",
      "\n",
      "CBSE - X\n",
      "Anand Vidya Vihar- Vadodara\n",
      "\n",
      "Result: 86%\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "May 2022\n",
      "\n",
      "March 2020\n",
      "\n",
      "Personal Finance Tracker Web App\n",
      "\n",
      "Feb 2024-April 2024\n",
      "\n",
      "Developed a Personal Finance Manager to help users organize and optimize their income, expenses,\n",
      "savings, and investments.\n",
      "Database Management: Configured and optimized MySQL database to store and retrieve user data.\n",
      "Utilized Flask to build a web application framework.\n",
      "User Interface Design: Created responsive and user-friendly web interfaces using HTML, CSS, and\n",
      "JS.\n",
      "Github : https://github.com/edwin16804/Finance-Tracker\n",
      "\n",
      "Smart Energy Management System using Ensemble Learning\n",
      "\n",
      "Feb 2024-May 2024\n",
      "\n",
      "Developed  an  SEMS  that  minimizes  energy  wastage  by  combining  the  best-performing  ensemble\n",
      "learning classifier based on accuracy.\n",
      "This  project  introduces  a  Smart  Energy  Management  System  (SEMS)  that  utilizes  ensemble  learning\n",
      "with  AdaBoost  and  Random  Forest  classifiers  to  predict  energy  waste  in  smart  homes  with  high\n",
      "accuracy. \n",
      "Github:https://github.com/edwin16804/Smart-Energy-Management-System-Using-Ensemble-Learning\n",
      "\n",
      "Hotel Management System\n",
      "\n",
      "Jan 2022-March 2022\n",
      "\n",
      "Developed a  Hotel Management System providing users with access to all essential features\n",
      "required for efficient hotel management, such as room booking, check-out, and reservation.\n",
      "Designed and developed the entire application using Python, used Tkinter for building the graphical\n",
      "user interface (GUI) of the application. used MySQL for managing and storing hotel data efficiently.\n",
      "Github : https://github.com/edwin16804/nocturnal-hotel-management-system\n",
      "\n",
      "AI powered job recommendation system\n",
      "\n",
      "System will connect job seekers with relevant job opportunities by leveraging machine\n",
      "learning and natural language processing (NLP)\n",
      "\n",
      "Intrusion Detection System in IOT devices using AI\n",
      "Automated threat detection and prevention\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Technical Skills: C/C++, Python, Java, ReactJS, JS, HTML,CSS, SQL\n",
      "Languages: English, Malayalam, Hindi, Spanish\n",
      "Awards/Activities: Won the TechnoVIT Ideathon for ‘TrashCanGo’ app idea\n",
      "\n",
      "Ongoing\n",
      "\n",
      "Ongoing\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "text=extract_text_from_pdf('C:/Users/EDWIN/OneDrive/Desktop/AI-job/AI-Job-Recommendation/DL-part/Edwin-Resume.pdf')\n",
    "#text=\"A leading real estate firm in New Jersey seeks an administrative Marketing Coordinator with graphic design experience. You'll work closely with the sales and executive teams, contributing to a fast-growing firm known for exceptional marketing. Responsibilities include agent assistance, graphic design and branding, and event planning. The ideal candidate is organized, creative, positive, and skilled in Adobe Creative Cloud and Microsoft Office Suite. This is a full-time position with a commitment to diversity and equal opportunity.\"\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDWIN CHAZHOOR 917069940011   Github  LinkedIn EDUCATION BTech Computer Science Engineering with specialization in AIML Vellore Institute of TechnologyChennai Jul 2022 Present CGPA 897 CBSE  XII Anand Vidya Vihar Vadodara Result 92 CBSE  X Anand Vidya Vihar Vadodara Result 86 PROJECTS May 2022 March 2020 Personal Finance Tracker Web App Feb 2024April 2024 Developed a Personal Finance Manager to help users organize and optimize their income expenses savings and investments Database Management Configured and optimized MySQL database to store and retrieve user data Utilized Flask to build a web application framework User Interface Design Created responsive and userfriendly web interfaces using HTML CSS and JS Github  Smart Energy Management System using Ensemble Learning Feb 2024May 2024 Developed an SEMS that minimizes energy wastage by combining the bestperforming ensemble learning classifier based on accuracy This project introduces a Smart Energy Management System SEMS that utilizes ensemble learning with AdaBoost and Random Forest classifiers to predict energy waste in smart homes with high accuracy Github Hotel Management System Jan 2022March 2022 Developed a Hotel Management System providing users with access to all essential features required for efficient hotel management such as room booking checkout and reservation Designed and developed the entire application using Python used Tkinter for building the graphical user interface GUI of the application used MySQL for managing and storing hotel data efficiently Github  AI powered job recommendation system System will connect job seekers with relevant job opportunities by leveraging machine learning and natural language processing NLP Intrusion Detection System in IOT devices using AI Automated threat detection and prevention SKILLS Technical Skills CC Python Java ReactJS JS HTMLCSS SQL Languages English Malayalam Hindi Spanish AwardsActivities Won the TechnoVIT Ideathon for ‘TrashCanGo’ app idea Ongoing Ongoing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Step 1: Remove newline characters (\\n)\n",
    "resume_text_cleaned = text.replace(\"\\n\", \" \")\n",
    "\n",
    "# Step 2: Remove URLs (links) that start with http, https, or www\n",
    "resume_text_cleaned = re.sub(r'(https?://\\S+|www\\.\\S+)', '', resume_text_cleaned)\n",
    "\n",
    "resume_text_cleaned = re.sub(r'\\S+@\\S+', '', resume_text_cleaned)\n",
    "\n",
    "# Step 3: Remove extra spaces\n",
    "resume_text_cleaned = re.sub(r'\\s+', ' ', resume_text_cleaned).strip()\n",
    "\n",
    "# Step 4: Remove punctuation, except '@'\n",
    "punctuation_to_remove = string.punctuation.replace('@', '')\n",
    "resume_text_cleaned = re.sub(r'[•' + re.escape(punctuation_to_remove) + ']', '', resume_text_cleaned)\n",
    "\n",
    "# Print the cleaned resume text\n",
    "print(resume_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Load the tokenizer and TensorFlow-based model\n",
    "model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples['judgement']]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Tokenize summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=150, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7773 [00:00<?, ? examples/s]C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  <Tip warning={true}>\n",
      "Map: 100%|██████████| 7773/7773 [04:03<00:00, 31.86 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:11<00:00, 17.98 examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train with TensorFlow's fit method\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mtraining_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     19\u001b[0m     tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mtraining_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_train_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtraining_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 22\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     76\u001b[0m         ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\")\n",
    "tokenized_data = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments for TensorFlow model\n",
    "training_args = {\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_dir\": \"./logs\",\n",
    "}\n",
    "\n",
    "# Train with TensorFlow's fit method\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=training_args['learning_rate']))\n",
    "model.fit(\n",
    "    tokenized_data['train'],\n",
    "    epochs=training_args['num_train_epochs'],\n",
    "    batch_size=training_args['per_device_train_batch_size'],\n",
    "    validation_data=tokenized_data['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\EDWIN\\.cache\\huggingface\\hub\\models--facebook--bart-large-xsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'facebook/bart-large-xsum' # Model\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint) # Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint) # Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartEncoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartDecoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\")\n",
    "\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "\n",
    "# Further split the test set into validation and test sets\n",
    "test_valid_split = train_test_split['test'].train_test_split(test_size=0.5)  # 50% validation, 50% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"judgement\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/6218 [00:00<?, ? examples/s]C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  <Tip warning={true}>\n",
      "Map: 100%|██████████| 6218/6218 [03:23<00:00, 30.53 examples/s]\n",
      "Map: 100%|██████████| 777/777 [00:24<00:00, 31.54 examples/s]\n",
      "Map: 100%|██████████| 778/778 [00:25<00:00, 30.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Training Dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 6218\n",
      "})\n",
      "Tokenized Validation Dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 777\n",
      "})\n",
      "Tokenized Test Dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 778\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created `split_dataset` containing train, validation, and test sets\n",
    "\n",
    "# Preprocess the training dataset\n",
    "tokenized_train = split_dataset['train'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['judgement','dataset_name', 'text', 'summary']  # Adjust these columns as needed\n",
    ")\n",
    "\n",
    "# Preprocess the validation dataset\n",
    "tokenized_val = split_dataset['validation'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['judgement','dataset_name', 'text', 'summary'] # Adjust these columns as needed\n",
    ")\n",
    "\n",
    "# Preprocess the test dataset\n",
    "tokenized_test = split_dataset['test'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['judgement','dataset_name', 'text', 'summary']  # Adjust these columns as needed\n",
    ")\n",
    "\n",
    "# Print the structure of the tokenized datasets for verification\n",
    "print(\"Tokenized Training Dataset:\", tokenized_train)\n",
    "print(\"Tokenized Validation Dataset:\", tokenized_val)\n",
    "print(\"Tokenized Test Dataset:\", tokenized_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.training_args_seq2seq because of the following error (look up to see its traceback):\ncannot import name 'is_torch_musa_available' from 'transformers.utils' (C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args_seq2seq.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_start_docstrings\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:31\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DebugOption\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     EvaluationStrategy,\n\u001b[0;32m     33\u001b[0m     FSDPOption,\n\u001b[0;32m     34\u001b[0m     HubStrategy,\n\u001b[0;32m     35\u001b[0m     IntervalStrategy,\n\u001b[0;32m     36\u001b[0m     SchedulerType,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m     40\u001b[0m     ExplicitEnum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     requires_backends,\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer_utils.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     ExplicitEnum,\n\u001b[0;32m     34\u001b[0m     is_psutil_available,\n\u001b[0;32m     35\u001b[0m     is_tf_available,\n\u001b[0;32m     36\u001b[0m     is_torch_available,\n\u001b[0;32m     37\u001b[0m     is_torch_cuda_available,\n\u001b[0;32m     38\u001b[0m     is_torch_mlu_available,\n\u001b[0;32m     39\u001b[0m     is_torch_mps_available,\n\u001b[0;32m     40\u001b[0m     is_torch_musa_available,\n\u001b[0;32m     41\u001b[0m     is_torch_npu_available,\n\u001b[0;32m     42\u001b[0m     is_torch_xla_available,\n\u001b[0;32m     43\u001b[0m     is_torch_xpu_available,\n\u001b[0;32m     44\u001b[0m     requires_backends,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torch_musa_available' from 'transformers.utils' (C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, AutoTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      6\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbart_samsum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     evaluation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.training_args_seq2seq because of the following error (look up to see its traceback):\ncannot import name 'is_torch_musa_available' from 'transformers.utils' (C:\\Users\\EDWIN\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = 'bart_samsum',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    seed = seed,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
